{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a470aa",
   "metadata": {},
   "source": [
    "# Feature Vector Data Preproecessing Pipeline\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8e7f8",
   "metadata": {},
   "source": [
    "## Step-1: Install dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395aac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Install / Import Dependencies\n",
    "!pip install -q scikit-learn pandas matplotlib numpy seaborn\n",
    "!pip install -q opendatasets  # For Kaggle dataset download\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999b25c",
   "metadata": {},
   "source": [
    "## Step-2: Load Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fde9f2",
   "metadata": {},
   "source": [
    "### Dowload Dataset from Kaggle -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e71702",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Selection - Choose which dataset to use\n",
    "DATASET_CHOICE = 'ember'  # Change to 'bodmas' to use BODMAS dataset\n",
    "\n",
    "# Kaggle dataset URLs\n",
    "EMBER_URL = 'https://www.kaggle.com/datasets/dhoogla/ember-2018-v2-features'\n",
    "BODMAS_URL = 'https://www.kaggle.com/datasets/dhoogla/bodmas'\n",
    "\n",
    "print(f\"Selected dataset: {DATASET_CHOICE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a1c3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download datasets from Kaggle (requires Kaggle API credentials)\n",
    "# Note: You need to upload your kaggle.json file to use this\n",
    "import os\n",
    "\n",
    "# Set your Kaggle credentials here\n",
    "os.environ['KAGGLE_USERNAME'] = 'razeenahmed10'\n",
    "os.environ['KAGGLE_KEY'] = '43efe04888a9c1878a4753108e73e0b9'\n",
    "\n",
    "# Then import or use Kaggle API functions that require authentication\n",
    "import opendatasets as od\n",
    "\n",
    "# Proceed with downloading datasets\n",
    "\n",
    "try:\n",
    "    # import opendatasets as od\n",
    "\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        print(\"Downloading EMBER 2018 v2 Features dataset...\")\n",
    "        od.download(EMBER_URL)\n",
    "        data_path = './ember-2018-v2-features'\n",
    "    else:\n",
    "        print(\"Downloading BODMAS dataset...\")\n",
    "        od.download(BODMAS_URL)\n",
    "        data_path = './bodmas'\n",
    "\n",
    "    print(f\"Dataset downloaded to: {data_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Please manually download the dataset or set up Kaggle API credentials\")\n",
    "\n",
    "    # Manual path setup (if you've already downloaded)\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        data_path = './ember-2018-v2-features'  # Adjust path as needed\n",
    "    else:\n",
    "        data_path = './bodmas'  # Adjust path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5501a4",
   "metadata": {},
   "source": [
    "### Load Dataset -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e0c93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ember_features_parquet(data_path):\n",
    "    \"\"\"Load EMBER 2018 v2 features from Kaggle dataset in parquet format\"\"\"\n",
    "    # Look for parquet files\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Loading features from: {parquet_files[0]}\")\n",
    "\n",
    "    # Load the features\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    # Find label column\n",
    "    label_cols = ['label', 'target', 'y', 'class']\n",
    "    label_col = None\n",
    "\n",
    "    for col in label_cols:\n",
    "        if col in df.columns:\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "    if label_col is None:\n",
    "        # Assume last column is label\n",
    "        label_col = df.columns[-1]\n",
    "        print(f\"No standard label column found, using: {label_col}\")\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "def load_bodmas_features_parquet(data_path):\n",
    "    \"\"\"Load BODMAS features from Kaggle dataset in parquet format\"\"\"\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Found parquet files: {parquet_files}\")\n",
    "\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        X = df.drop(columns=['label'])\n",
    "        y = df['label']\n",
    "    else:\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "print(f\"Loading {DATASET_CHOICE.upper()} dataset...\")\n",
    "\n",
    "if DATASET_CHOICE == 'ember':\n",
    "    X_raw, y_raw = load_ember_features_parquet(data_path)\n",
    "else:\n",
    "    X_raw, y_raw = load_bodmas_features_parquet(data_path)\n",
    "\n",
    "print(f'Raw {DATASET_CHOICE.upper()} shape: X={X_raw.shape}, y={y_raw.shape}')\n",
    "print(f'Feature dimensions: {X_raw.shape[1]}')\n",
    "print(f'Label distribution: {np.unique(y_raw, return_counts=True)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd97f1f",
   "metadata": {},
   "source": [
    "## Step-2.5: Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f900e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing and Cleaning\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"Missing values in features: {np.sum(np.isnan(X_raw))}\")\n",
    "if np.sum(np.isnan(X_raw)) > 0:\n",
    "    # Replace NaN with 0 or median\n",
    "    X_raw = np.nan_to_num(X_raw, nan=0.0)\n",
    "    print(\"Replaced NaN values with 0\")\n",
    "\n",
    "# Handle infinite values\n",
    "inf_mask = ~np.isfinite(X_raw)\n",
    "if np.sum(inf_mask) > 0:\n",
    "    X_raw[inf_mask] = 0\n",
    "    print(f\"Replaced {np.sum(inf_mask)} infinite values with 0\")\n",
    "\n",
    "# Encode labels to binary (0, 1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "\n",
    "print(f\"Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "print(f\"Encoded label distribution: {np.unique(y_encoded, return_counts=True)}\")\n",
    "\n",
    "# Filter out unlabeled data (if any)\n",
    "if -1 in y_encoded:\n",
    "    mask = y_encoded != -1\n",
    "    X_raw = X_raw[mask]\n",
    "    y_encoded = y_encoded[mask]\n",
    "    print(f\"Filtered data shape: X={X_raw.shape}, y={y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6daf1a",
   "metadata": {},
   "source": [
    "## Step-3: Train/Test Split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1fa9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Train/Test Split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n",
    "print('Train labels:', np.unique(y_train, return_counts=True))\n",
    "print('Test labels:', np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3efc7b",
   "metadata": {},
   "source": [
    "## Step-4: Standardization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7def18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Standardization (mean=0, std=1)\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print(f\"After standardization:\")\n",
    "print(f\"Train mean: {X_train_std.mean():.6f}, std: {X_train_std.std():.6f}\")\n",
    "print(f\"Test mean: {X_test_std.mean():.6f}, std: {X_test_std.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c965513",
   "metadata": {},
   "source": [
    "## Step-5: PCA30 Transformation of Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96de863",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. PCA → 30 Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Applying PCA (30 components)...\")\n",
    "pca = PCA(n_components=30, random_state=42, svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "print('After PCA:', X_train_pca.shape)\n",
    "print(f'Variance explained by 30 components: {pca.explained_variance_ratio_.sum()*100:.2f}%')\n",
    "print(f'Top 10 component variances: {pca.explained_variance_ratio_[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a52af",
   "metadata": {},
   "source": [
    "## Step-6: Labelling of the Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb2006",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Prepare Labels for QCNN {-1, 1}\n",
    "print(\"Converting labels to {-1, +1} format for QCNN...\")\n",
    "y_train_q = np.where(y_train == 1, 1, -1).astype(np.int8)\n",
    "y_test_q = np.where(y_test == 1, 1, -1).astype(np.int8)\n",
    "\n",
    "print(f'QCNN labels - Train: {np.unique(y_train_q, return_counts=True)}')\n",
    "print(f'QCNN labels - Test: {np.unique(y_test_q, return_counts=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ad1db",
   "metadata": {},
   "source": [
    "## Step-7: Visualization of the Components\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f6d2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 8. Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# PCA Explained Variance\n",
    "axes[0, 0].plot(range(1, 31), np.cumsum(pca.explained_variance_ratio_), marker='o', linewidth=2)\n",
    "axes[0, 0].set_title('Cumulative Explained Variance (30 PCs)')\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Cumulative Variance Explained')\n",
    "axes[0, 0].axhline(0.8, linestyle='--', color='red', alpha=0.7, label='80%')\n",
    "axes[0, 0].axhline(0.9, linestyle='--', color='orange', alpha=0.7, label='90%')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Individual Component Variance\n",
    "axes[0, 1].bar(range(1, 31), pca.explained_variance_ratio_, alpha=0.7)\n",
    "axes[0, 1].set_title('Individual Component Variance')\n",
    "axes[0, 1].set_xlabel('Principal Component')\n",
    "axes[0, 1].set_ylabel('Variance Explained')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PC1 vs PC2 Scatter Plot\n",
    "sample_size = min(5000, len(X_train_pca))  # Sample for visualization\n",
    "sample_idx = np.random.choice(len(X_train_pca), sample_size, replace=False)\n",
    "scatter = axes[1, 0].scatter(X_train_pca[sample_idx, 0], X_train_pca[sample_idx, 1],\n",
    "                            c=y_train[sample_idx], cmap='coolwarm', s=10, alpha=0.6)\n",
    "axes[1, 0].set_title(f'PC1 vs PC2 ({sample_size} samples)')\n",
    "axes[1, 0].set_xlabel('Principal Component 1')\n",
    "axes[1, 0].set_ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter, ax=axes[1, 0])\n",
    "\n",
    "# Quantum Normalized Distribution\n",
    "axes[1, 1].hist(X_train_q[:, 0], bins=50, alpha=0.7, label='PC1', density=True)\n",
    "axes[1, 1].hist(X_train_q[:, 1], bins=50, alpha=0.7, label='PC2', density=True)\n",
    "axes[1, 1].set_title('Quantum Normalized Distribution (PC1, PC2)')\n",
    "axes[1, 1].set_xlabel('Quantum Angle (radians)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].axvline(np.pi, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\n=== PREPROCESSING SUMMARY ===\")\n",
    "print(f\"Dataset: {DATASET_CHOICE.upper()}\")\n",
    "print(f\"Original features: {X_raw.shape[1]}\")\n",
    "print(f\"PCA components: {X_train_pca.shape[1]}\")\n",
    "print(f\"Variance retained: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "print(f\"Training samples: {X_train_q.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_q.shape[0]}\")\n",
    "print(f\"Quantum range: [0, π] = [0, {np.pi:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733d4f0",
   "metadata": {},
   "source": [
    "## Step-8: Save Processed Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ded3ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 9. Save Processed Data\n",
    "output_prefix = f'{DATASET_CHOICE.upper()}_PCA30'\n",
    "\n",
    "# Save as compressed NPZ files\n",
    "np.savez_compressed(f'{output_prefix}_train.npz', X=X_train_q, y=y_train_q)\n",
    "np.savez_compressed(f'{output_prefix}_test.npz', X=X_test_q, y=y_test_q)\n",
    "\n",
    "# Save preprocessing objects for later use\n",
    "import pickle\n",
    "with open(f'{output_prefix}_preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'label_encoder': le,\n",
    "        'quantum_params': {'X_min': X_min, 'X_max': X_max, 'range_vals': range_vals}\n",
    "    }, f)\n",
    "\n",
    "# Save summary report\n",
    "summary_report = f\"\"\"# {DATASET_CHOICE.upper()} PCA30 Preprocessing Report\n",
    "\n",
    "## Dataset Information\n",
    "- Dataset: {DATASET_CHOICE.upper()}\n",
    "- Original features: {X_raw.shape[1]}\n",
    "- Original samples: {X_raw.shape[0]}\n",
    "- Training samples: {X_train_q.shape[0]}\n",
    "- Test samples: {X_test_q.shape[0]}\n",
    "\n",
    "## PCA Analysis\n",
    "- Components: {X_train_pca.shape[1]}\n",
    "- Variance retained: {pca.explained_variance_ratio_.sum()*100:.2f}%\n",
    "- Top 5 component variances: {pca.explained_variance_ratio_[:5].tolist()}\n",
    "\n",
    "## Label Distribution\n",
    "- Train: {dict(zip(*np.unique(y_train_q, return_counts=True)))}\n",
    "- Test: {dict(zip(*np.unique(y_test_q, return_counts=True)))}\n",
    "\n",
    "## Quantum Normalization\n",
    "- Range: [0, π] = [0, {np.pi:.4f}]\n",
    "- Train min: {X_train_q.min():.6f}\n",
    "- Train max: {X_train_q.max():.6f}\n",
    "\n",
    "## Files Generated\n",
    "- {output_prefix}_train.npz\n",
    "- {output_prefix}_test.npz\n",
    "- {output_prefix}_preprocessing_objects.pkl\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{output_prefix}_report.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n=== FILES SAVED ===\")\n",
    "print(f\"✓ {output_prefix}_train.npz\")\n",
    "print(f\"✓ {output_prefix}_test.npz\")\n",
    "print(f\"✓ {output_prefix}_preprocessing_objects.pkl\")\n",
    "print(f\"✓ {output_prefix}_report.md\")\n",
    "print(\"\\nPreprocessing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
