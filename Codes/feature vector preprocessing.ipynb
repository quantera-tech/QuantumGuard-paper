{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a470aa",
   "metadata": {},
   "source": [
    "# Feature Vector Data Preproecessing Pipeline\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8e7f8",
   "metadata": {},
   "source": [
    "## Step-1: Install dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395aac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Install / Import Dependencies\n",
    "!pip install -q scikit-learn pandas matplotlib numpy seaborn\n",
    "!pip install -q opendatasets  # For Kaggle dataset download\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999b25c",
   "metadata": {},
   "source": [
    "## Step-2: Load Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fde9f2",
   "metadata": {},
   "source": [
    "### Dowload Dataset from Kaggle -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e71702",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Selection - Choose which dataset to use\n",
    "DATASET_CHOICE = 'ember'  # Change to 'bodmas' to use BODMAS dataset\n",
    "\n",
    "# Kaggle dataset URLs\n",
    "EMBER_URL = 'https://www.kaggle.com/datasets/dhoogla/ember-2018-v2-features'\n",
    "BODMAS_URL = 'https://www.kaggle.com/datasets/dhoogla/bodmas'\n",
    "\n",
    "print(f\"Selected dataset: {DATASET_CHOICE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a1c3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download datasets from Kaggle (requires Kaggle API credentials)\n",
    "# Note: You need to upload your kaggle.json file to use this\n",
    "import os\n",
    "\n",
    "# Set your Kaggle credentials here\n",
    "os.environ['KAGGLE_USERNAME'] = 'razeenahmed10'\n",
    "os.environ['KAGGLE_KEY'] = '43efe04888a9c1878a4753108e73e0b9'\n",
    "\n",
    "# Then import or use Kaggle API functions that require authentication\n",
    "import opendatasets as od\n",
    "\n",
    "# Proceed with downloading datasets\n",
    "\n",
    "try:\n",
    "    # import opendatasets as od\n",
    "\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        print(\"Downloading EMBER 2018 v2 Features dataset...\")\n",
    "        od.download(EMBER_URL)\n",
    "        data_path = './ember-2018-v2-features'\n",
    "    else:\n",
    "        print(\"Downloading BODMAS dataset...\")\n",
    "        od.download(BODMAS_URL)\n",
    "        data_path = './bodmas'\n",
    "\n",
    "    print(f\"Dataset downloaded to: {data_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Please manually download the dataset or set up Kaggle API credentials\")\n",
    "\n",
    "    # Manual path setup (if you've already downloaded)\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        data_path = './ember-2018-v2-features'  # Adjust path as needed\n",
    "    else:\n",
    "        data_path = './bodmas'  # Adjust path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5501a4",
   "metadata": {},
   "source": [
    "### Load Dataset -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e0c93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ember_features_parquet(data_path):\n",
    "    \"\"\"Load EMBER 2018 v2 features from Kaggle dataset in parquet format\"\"\"\n",
    "    # Look for parquet files\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Loading features from: {parquet_files[0]}\")\n",
    "\n",
    "    # Load the features\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    # Find label column\n",
    "    label_cols = ['label', 'target', 'y', 'class']\n",
    "    label_col = None\n",
    "\n",
    "    for col in label_cols:\n",
    "        if col in df.columns:\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "    if label_col is None:\n",
    "        # Assume last column is label\n",
    "        label_col = df.columns[-1]\n",
    "        print(f\"No standard label column found, using: {label_col}\")\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "def load_bodmas_features_parquet(data_path):\n",
    "    \"\"\"Load BODMAS features from Kaggle dataset in parquet format\"\"\"\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Found parquet files: {parquet_files}\")\n",
    "\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        X = df.drop(columns=['label'])\n",
    "        y = df['label']\n",
    "    else:\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "print(f\"Loading {DATASET_CHOICE.upper()} dataset...\")\n",
    "\n",
    "if DATASET_CHOICE == 'ember':\n",
    "    X_raw, y_raw = load_ember_features_parquet(data_path)\n",
    "else:\n",
    "    X_raw, y_raw = load_bodmas_features_parquet(data_path)\n",
    "\n",
    "print(f'Raw {DATASET_CHOICE.upper()} shape: X={X_raw.shape}, y={y_raw.shape}')\n",
    "print(f'Feature dimensions: {X_raw.shape[1]}')\n",
    "print(f'Label distribution: {np.unique(y_raw, return_counts=True)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6daf1a",
   "metadata": {},
   "source": [
    "## Step-3: Train/Test Split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1fa9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Train/Test Split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.20, random_state=42, stratify=y_raw)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3efc7b",
   "metadata": {},
   "source": [
    "## Step-4: Standardization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7def18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler(copy=False)\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c965513",
   "metadata": {},
   "source": [
    "## Step-5: PCA30 Transformation of Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96de863",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. PCA â†’ 30 Principal Components\n",
    "pca = PCA(n_components=30, random_state=42, svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "print('After PCA:', X_train_pca.shape)\n",
    "print('Variance kept:', pca.explained_variance_ratio_.sum()*100, '%')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
