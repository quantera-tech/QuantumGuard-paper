{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a470aa",
   "metadata": {},
   "source": [
    "# Feature Vector Data Preproecessing Pipeline\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8e7f8",
   "metadata": {},
   "source": [
    "## Step-1: Install dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395aac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Install / Import Dependencies\n",
    "!pip install -q scikit-learn pandas matplotlib numpy seaborn\n",
    "!pip install -q opendatasets  # For Kaggle dataset download\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999b25c",
   "metadata": {},
   "source": [
    "## Step-2: Load Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fde9f2",
   "metadata": {},
   "source": [
    "### Dowload Dataset from Kaggle -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e71702",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Selection - Choose which dataset to use\n",
    "DATASET_CHOICE = 'ember'  # Change to 'bodmas' to use BODMAS dataset\n",
    "\n",
    "# Kaggle dataset URLs\n",
    "EMBER_URL = 'https://www.kaggle.com/datasets/dhoogla/ember-2018-v2-features'\n",
    "BODMAS_URL = 'https://www.kaggle.com/datasets/dhoogla/bodmas'\n",
    "\n",
    "print(f\"Selected dataset: {DATASET_CHOICE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a1c3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download datasets from Kaggle (requires Kaggle API credentials)\n",
    "# Note: You need to upload your kaggle.json file to use this\n",
    "import os\n",
    "\n",
    "# Set your Kaggle credentials here\n",
    "os.environ['KAGGLE_USERNAME'] = 'razeenahmed10'\n",
    "os.environ['KAGGLE_KEY'] = '43efe04888a9c1878a4753108e73e0b9'\n",
    "\n",
    "# Then import or use Kaggle API functions that require authentication\n",
    "import opendatasets as od\n",
    "\n",
    "# Proceed with downloading datasets\n",
    "\n",
    "try:\n",
    "    # import opendatasets as od\n",
    "\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        print(\"Downloading EMBER 2018 v2 Features dataset...\")\n",
    "        od.download(EMBER_URL)\n",
    "        data_path = './ember-2018-v2-features'\n",
    "    else:\n",
    "        print(\"Downloading BODMAS dataset...\")\n",
    "        od.download(BODMAS_URL)\n",
    "        data_path = './bodmas'\n",
    "\n",
    "    print(f\"Dataset downloaded to: {data_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Please manually download the dataset or set up Kaggle API credentials\")\n",
    "\n",
    "    # Manual path setup (if you've already downloaded)\n",
    "    if DATASET_CHOICE == 'ember':\n",
    "        data_path = './ember-2018-v2-features'  # Adjust path as needed\n",
    "    else:\n",
    "        data_path = './bodmas'  # Adjust path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5501a4",
   "metadata": {},
   "source": [
    "### Load Dataset -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e0c93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ember_features_parquet(data_path):\n",
    "    \"\"\"Load EMBER 2018 v2 features from Kaggle dataset in parquet format\"\"\"\n",
    "    # Look for parquet files\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Loading features from: {parquet_files[0]}\")\n",
    "\n",
    "    # Load the features\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    # Find label column\n",
    "    label_cols = ['label', 'target', 'y', 'class']\n",
    "    label_col = None\n",
    "\n",
    "    for col in label_cols:\n",
    "        if col in df.columns:\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "    if label_col is None:\n",
    "        # Assume last column is label\n",
    "        label_col = df.columns[-1]\n",
    "        print(f\"No standard label column found, using: {label_col}\")\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = df.drop(columns=[label_col])\n",
    "    y = df[label_col]\n",
    "\n",
    "    # Remove non-numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "def load_bodmas_features_parquet(data_path):\n",
    "    \"\"\"Load BODMAS features from Kaggle dataset in parquet format\"\"\"\n",
    "    parquet_files = glob.glob(os.path.join(data_path, '*.parquet'))\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {data_path}\")\n",
    "\n",
    "    print(f\"Found parquet files: {parquet_files}\")\n",
    "\n",
    "    df = pd.read_parquet(parquet_files[0])\n",
    "\n",
    "    if 'label' in df.columns:\n",
    "        X = df.drop(columns=['label'])\n",
    "        y = df['label']\n",
    "    else:\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X = X[numeric_cols]\n",
    "\n",
    "    return X.values, y.values\n",
    "\n",
    "\n",
    "print(f\"Loading {DATASET_CHOICE.upper()} dataset...\")\n",
    "\n",
    "if DATASET_CHOICE == 'ember':\n",
    "    X_raw, y_raw = load_ember_features_parquet(data_path)\n",
    "else:\n",
    "    X_raw, y_raw = load_bodmas_features_parquet(data_path)\n",
    "\n",
    "print(f'Raw {DATASET_CHOICE.upper()} shape: X={X_raw.shape}, y={y_raw.shape}')\n",
    "print(f'Feature dimensions: {X_raw.shape[1]}')\n",
    "print(f'Label distribution: {np.unique(y_raw, return_counts=True)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd97f1f",
   "metadata": {},
   "source": [
    "## Step-2.5: Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f900e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing and Cleaning\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"Missing values in features: {np.sum(np.isnan(X_raw))}\")\n",
    "if np.sum(np.isnan(X_raw)) > 0:\n",
    "    # Replace NaN with 0 or median\n",
    "    X_raw = np.nan_to_num(X_raw, nan=0.0)\n",
    "    print(\"Replaced NaN values with 0\")\n",
    "\n",
    "# Handle infinite values\n",
    "inf_mask = ~np.isfinite(X_raw)\n",
    "if np.sum(inf_mask) > 0:\n",
    "    X_raw[inf_mask] = 0\n",
    "    print(f\"Replaced {np.sum(inf_mask)} infinite values with 0\")\n",
    "\n",
    "# Encode labels to binary (0, 1)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "\n",
    "print(f\"Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "print(f\"Encoded label distribution: {np.unique(y_encoded, return_counts=True)}\")\n",
    "\n",
    "# Filter out unlabeled data (if any)\n",
    "if -1 in y_encoded:\n",
    "    mask = y_encoded != -1\n",
    "    X_raw = X_raw[mask]\n",
    "    y_encoded = y_encoded[mask]\n",
    "    print(f\"Filtered data shape: X={X_raw.shape}, y={y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6daf1a",
   "metadata": {},
   "source": [
    "## Step-3: Train/Test Split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1fa9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Train/Test Split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n",
    "print('Train labels:', np.unique(y_train, return_counts=True))\n",
    "print('Test labels:', np.unique(y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3efc7b",
   "metadata": {},
   "source": [
    "## Step-4: Standardization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7def18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Standardization (mean=0, std=1)\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "print(f\"After standardization:\")\n",
    "print(f\"Train mean: {X_train_std.mean():.6f}, std: {X_train_std.std():.6f}\")\n",
    "print(f\"Test mean: {X_test_std.mean():.6f}, std: {X_test_std.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c965513",
   "metadata": {},
   "source": [
    "## Step-5: PCA30 Transformation of Feature Vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96de863",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. PCA → 30 Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Applying PCA (30 components)...\")\n",
    "pca = PCA(n_components=30, random_state=42, svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "print('After PCA:', X_train_pca.shape)\n",
    "print(f'Variance explained by 30 components: {pca.explained_variance_ratio_.sum()*100:.2f}%')\n",
    "print(f'Top 10 component variances: {pca.explained_variance_ratio_[:10]}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
